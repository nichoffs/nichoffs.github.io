<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mini-Projects on Nic Hoffs Blog</title>
    <link>http://localhost:1313/post/mini_projects/</link>
    <description>Recent content in Mini-Projects on Nic Hoffs Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2024 16:25:35 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/post/mini_projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TinyGrad GPT2 à la Karpathy - Part 1</title>
      <link>http://localhost:1313/post/mini_projects/gpt2_p1/</link>
      <pubDate>Tue, 11 Jun 2024 16:25:35 +0200</pubDate>
      <guid>http://localhost:1313/post/mini_projects/gpt2_p1/</guid>
      <description>Whenever Andrej Karpathy releases a new YouTube video, the only moral thing to do is drop all responsibilites and replicate it in TinyGrad. This released yesterday (June 10) and is over 4 hours long (I&amp;rsquo;m salivating), so I&amp;rsquo;ve only finished the first part. I anticipate the later parts of the video will include more Torch-specific optimizations, which will make things a bit more difficult.&#xA;Setting up inference with pre-trained weights GPT2 has four models from 124M to 1.</description>
    </item>
    <item>
      <title>TinyGrad GPT2 à la Karpathy - Part 2</title>
      <link>http://localhost:1313/post/mini_projects/gpt2_p2/</link>
      <pubDate>Tue, 11 Jun 2024 16:25:35 +0200</pubDate>
      <guid>http://localhost:1313/post/mini_projects/gpt2_p2/</guid>
      <description>As expected, this part of the series has been more difficult. Karpathy makes use of lots of relatively niche, PyTorch specific functions that don&amp;rsquo;t have built-in support in TinyGrad. As a result, I need to be more methodical about how I approach this and rigorously confirm that things are working as expected.&#xA;I&amp;rsquo;ll start with the code we left off on in part 1.&#xA;from tinygrad import Tensor, dtypes from tinygrad.</description>
    </item>
    <item>
      <title>XOR</title>
      <link>http://localhost:1313/post/mini_projects/xor/</link>
      <pubDate>Sat, 08 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/mini_projects/xor/</guid>
      <description>The tiniest project &amp;ndash; teaching a neural net to XOR. Obviously, I&amp;rsquo;m doing this in tinygrad because it&amp;rsquo;s tiny.&#xA;A B A XOR B 0 0 0 0 1 1 1 0 1 1 1 0 First, imports.&#xA;from tinygrad import Tensor, TinyJit from tinygrad.nn.optim import SGD import numpy as np from tqdm import tqdm, trange import matplotlib.pyplot as plt Next, dataset. It&amp;rsquo;s the same as the truth table.&#xA;x = Tensor([[0,0],[0,1],[1,0],[1,1]]) y = Tensor([[0],[1],[1],[0]]) x.</description>
    </item>
  </channel>
</rss>
