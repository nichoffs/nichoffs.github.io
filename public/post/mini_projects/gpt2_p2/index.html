<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>TinyGrad GPT2 à la Karpathy - Part 2 | Nic Hoffs Blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post/">Posts</a></li>
      
      <li><a href="/about/">About</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">TinyGrad GPT2 à la Karpathy - Part 2</span></h1>

<h2 class="date">2024/06/11</h2>
</div>

<main>
<p>As expected, this part of the series has been more difficult. Karpathy makes use of lots of relatively niche, PyTorch specific functions that don&rsquo;t have built-in support in TinyGrad. As a result, I need to be more methodical about how I approach this and rigorously confirm that things are working as expected.</p>
<p>I&rsquo;ll start with the code we left off on in part 1.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tinygrad <span style="color:#f92672">import</span> Tensor, dtypes
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tinygrad.nn <span style="color:#f92672">import</span> Embedding, Linear, LayerNorm
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tinygrad.nn.state <span style="color:#f92672">import</span> torch_load, load_state_dict, get_state_dict, get_parameters
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tqdm <span style="color:#f92672">import</span> tqdm, trange
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tinygrad.nn.optim <span style="color:#f92672">import</span> AdamW
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> dataclasses <span style="color:#f92672">import</span> dataclass
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tinygrad.helpers <span style="color:#f92672">import</span> fetch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tiktoken
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT2Config</span>:
</span></span><span style="display:flex;"><span>    block_size: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>
</span></span><span style="display:flex;"><span>    vocab_size: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">50257</span>
</span></span><span style="display:flex;"><span>    n_layer: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>    n_head: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>    n_embd: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">768</span>
</span></span><span style="display:flex;"><span>    norm_eps: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT2Small</span>(GPT2Config):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT2Medium</span>(GPT2Config):
</span></span><span style="display:flex;"><span>    n_layer: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">24</span>
</span></span><span style="display:flex;"><span>    n_head: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>    n_embd: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT2Large</span>(GPT2Config):
</span></span><span style="display:flex;"><span>    n_layer: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">36</span>
</span></span><span style="display:flex;"><span>    n_head: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>    n_embd: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1280</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT2XL</span>(GPT2Config):
</span></span><span style="display:flex;"><span>    n_layer: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">48</span>
</span></span><span style="display:flex;"><span>    n_head: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>    n_embd: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1600</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>MODEL_CONFIGS <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;gpt2&#39;</span>: GPT2Small,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;gpt2-medium&#39;</span>: GPT2Medium,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;gpt2-large&#39;</span>: GPT2Large,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;gpt2-xl&#39;</span>: GPT2XL
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MLP</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config : GPT2Config):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_fc <span style="color:#f92672">=</span> Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd<span style="color:#f92672">*</span><span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> Linear(config<span style="color:#f92672">.</span>n_embd<span style="color:#f92672">*</span><span style="color:#ae81ff">4</span>, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_fc(x)<span style="color:#f92672">.</span>gelu()
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Attention</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config: GPT2Config):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_attn <span style="color:#f92672">=</span> Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, x):
</span></span><span style="display:flex;"><span>        B,T,C <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_attn(x)<span style="color:#f92672">.</span>split(C, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e">#(B,T,3C) -&gt; (B,T,C) x 3</span>
</span></span><span style="display:flex;"><span>        split_heads <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: x<span style="color:#f92672">.</span>view(B, T, self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>n_head, self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>n_embd<span style="color:#f92672">//</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>n_head)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> map(split_heads, (q,k,v))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> q<span style="color:#f92672">.</span>scaled_dot_product_attention(k, v, is_causal<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, T, C)
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TransformerBlock</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config : GPT2Config):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln_1 <span style="color:#f92672">=</span> LayerNorm(config<span style="color:#f92672">.</span>n_embd, eps<span style="color:#f92672">=</span>config<span style="color:#f92672">.</span>norm_eps)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln_2 <span style="color:#f92672">=</span> LayerNorm(config<span style="color:#f92672">.</span>n_embd, eps<span style="color:#f92672">=</span>config<span style="color:#f92672">.</span>norm_eps)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> Attention(config)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mlp <span style="color:#f92672">=</span> MLP(config)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>attn(self<span style="color:#f92672">.</span>ln_1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>mlp(self<span style="color:#f92672">.</span>ln_2(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT2</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, config : GPT2Config <span style="color:#f92672">=</span> GPT2Small):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>config <span style="color:#f92672">=</span> config
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>wte <span style="color:#f92672">=</span> Embedding(config<span style="color:#f92672">.</span>vocab_size, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>wpe <span style="color:#f92672">=</span> Embedding(config<span style="color:#f92672">.</span>block_size, config<span style="color:#f92672">.</span>n_embd)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>h <span style="color:#f92672">=</span> [TransformerBlock(config) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(config<span style="color:#f92672">.</span>n_layer)]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln_f <span style="color:#f92672">=</span> LayerNorm(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>norm_eps)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head <span style="color:#f92672">=</span> Linear(config<span style="color:#f92672">.</span>n_embd, config<span style="color:#f92672">.</span>vocab_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># tie weights - HUGE SAVINGS</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wte<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __call__(self, idx, targets<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        B,T <span style="color:#f92672">=</span> idx<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> T <span style="color:#f92672">&lt;=</span> self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Cannot forward, model block size is </span><span style="color:#e6db74">{</span>self<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>block_size<span style="color:#e6db74">}</span><span style="color:#e6db74"> but got sequence of length </span><span style="color:#e6db74">{</span>T<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        pos <span style="color:#f92672">=</span> Tensor<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, T, dtype<span style="color:#f92672">=</span>dtypes<span style="color:#f92672">.</span>long) <span style="color:#75715e"># (T,)</span>
</span></span><span style="display:flex;"><span>        pos_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wpe(pos) <span style="color:#75715e"># (T,) -&gt; (T,C)</span>
</span></span><span style="display:flex;"><span>        tok_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wte(idx) <span style="color:#75715e"># (B,T) -&gt; (B,T,C)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tok_emb <span style="color:#f92672">+</span> pos_emb
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>sequential(self<span style="color:#f92672">.</span>h)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln_f(x)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x) <span style="color:#75715e"># (B,T,C) -&gt; (B,T,V)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> targets <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> logits<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>sparse_categorical_crossentropy(targets<span style="color:#f92672">.</span>flatten())
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> logits, loss<span style="color:#f92672">.</span>realize()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits, <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build</span>(MODEL_NAME):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        weights <span style="color:#f92672">=</span> torch_load(fetch(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;https://huggingface.co/</span><span style="color:#e6db74">{</span>MODEL_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">/resolve/main/pytorch_model.bin&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        transposed <span style="color:#f92672">=</span> (<span style="color:#e6db74">&#39;attn.c_attn.weight&#39;</span>, <span style="color:#e6db74">&#39;attn.c_proj.weight&#39;</span>, <span style="color:#e6db74">&#39;mlp.c_fc.weight&#39;</span>, <span style="color:#e6db74">&#39;mlp.c_proj.weight&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> weights:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> k<span style="color:#f92672">.</span>endswith(transposed):
</span></span><span style="display:flex;"><span>                weights[k] <span style="color:#f92672">=</span> weights[k]<span style="color:#f92672">.</span>T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        weights[<span style="color:#e6db74">&#39;lm_head.weight&#39;</span>] <span style="color:#f92672">=</span> weights[<span style="color:#e6db74">&#39;wte.weight&#39;</span>]
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> GPT2(MODEL_CONFIGS[MODEL_NAME])
</span></span><span style="display:flex;"><span>        load_state_dict(model, weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DataLoaderLite</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, B, T, file_path):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>B<span style="color:#f92672">=</span>B
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>T<span style="color:#f92672">=</span>T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: x<span style="color:#f92672">.</span>view(B,T)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> open(file_path, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>            text <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        enc <span style="color:#f92672">=</span> tiktoken<span style="color:#f92672">.</span>get_encoding(<span style="color:#e6db74">&#39;gpt2&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        tokens <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>encode(text)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>tokens <span style="color:#f92672">=</span> Tensor(tokens, dtype<span style="color:#f92672">=</span>dtypes<span style="color:#f92672">.</span>long)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;loaded </span><span style="color:#e6db74">{</span>len(self<span style="color:#f92672">.</span>tokens)<span style="color:#e6db74">}</span><span style="color:#e6db74"> tokens&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;1 epoch = </span><span style="color:#e6db74">{</span>len(self<span style="color:#f92672">.</span>tokens) <span style="color:#f92672">//</span> (B<span style="color:#f92672">*</span>T)<span style="color:#e6db74">}</span><span style="color:#e6db74"> batches&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">next_batch</span>(self):
</span></span><span style="display:flex;"><span>        B, T <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>B, self<span style="color:#f92672">.</span>T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        buf <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>tokens[self<span style="color:#f92672">.</span>current_position:self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">+</span> B<span style="color:#f92672">*</span>T<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>batch(buf[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>batch(buf[<span style="color:#ae81ff">1</span>:])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">+=</span> B<span style="color:#f92672">*</span>T
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">+</span> (B<span style="color:#f92672">*</span>T<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">&gt;</span> len(self<span style="color:#f92672">.</span>tokens):
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#34;read entire document, resetting position...&#34;</span>)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>current_position <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x,y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Tensor<span style="color:#f92672">.</span>training <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>Tensor<span style="color:#f92672">.</span>no_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT2(GPT2Small)
</span></span><span style="display:flex;"><span>optim <span style="color:#f92672">=</span> AdamW(get_parameters(model), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">3e-4</span>)
</span></span><span style="display:flex;"><span>dl <span style="color:#f92672">=</span> DataLoaderLite(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">32</span>, <span style="color:#e6db74">&#34;datasets/shake.txt&#34;</span>)
</span></span><span style="display:flex;"><span>losses <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> (t <span style="color:#f92672">:=</span> trange(<span style="color:#ae81ff">100</span>)):
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> dl<span style="color:#f92672">.</span>next_batch()
</span></span><span style="display:flex;"><span>    optim<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    logits, loss <span style="color:#f92672">=</span> model(x,y)
</span></span><span style="display:flex;"><span>    losses<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>numpy())
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optim<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    t<span style="color:#f92672">.</span>set_description(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;train loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>numpy()<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div>
</main>

  <footer>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script src="//cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js"
	defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<script>
	document.addEventListener("DOMContentLoaded", function () {
		renderMathInElement(document.body, {
			delimiters: [
				{left: "$$", right: "$$", display: true},
				{left: "\\(", right: "\\)", display: false},
				{left: "$", right: "$", display: false}
			]
		});
	});
</script>

  
  </footer>
  </body>
</html>

