<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Nic Hoffs Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Nic Hoffs Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Jul 2024 00:26:18 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Teenygrad</title>
      <link>http://localhost:1313/post/tinygrad/teenygrad/</link>
      <pubDate>Fri, 05 Jul 2024 00:26:18 +0200</pubDate>
      <guid>http://localhost:1313/post/tinygrad/teenygrad/</guid>
      <description>TeenyGrad is TinyGrad&amp;rsquo;s younger sibling.</description>
    </item>
    <item>
      <title>Why Tiny?</title>
      <link>http://localhost:1313/post/tinygrad/the_history/</link>
      <pubDate>Thu, 04 Jul 2024 21:51:02 +0200</pubDate>
      <guid>http://localhost:1313/post/tinygrad/the_history/</guid>
      <description>TinyPhilosophy TinyGrad is a tensor automatic differentiation library created by George Hotz in 2020. It&amp;rsquo;s been described (by TinyCorp) as a middleground between Anrej Karpathy&amp;rsquo;s famous micrograd project and full-blown PyTorch. It offers both the beautiful simplicity, leanness, and ease of development of micrograd, and almost all the speed and functionality of PyTorch.&#xA;An interesting features of TinyGrad&amp;rsquo;s development is a continued, explicit constraint on the line count (~8000 LOC today).</description>
    </item>
    <item>
      <title>TinyGrad GPT2 à la Karpathy - Part 1</title>
      <link>http://localhost:1313/post/mini_projects/gpt2_p1/</link>
      <pubDate>Tue, 11 Jun 2024 16:25:35 +0200</pubDate>
      <guid>http://localhost:1313/post/mini_projects/gpt2_p1/</guid>
      <description>Whenever Andrej Karpathy releases a new YouTube video, the only moral thing to do is drop all responsibilites and replicate it in TinyGrad. This released yesterday (June 10) and is over 4 hours long (I&amp;rsquo;m salivating), so I&amp;rsquo;ve only finished the first part. I anticipate the later parts of the video will include more Torch-specific optimizations, which will make things a bit more difficult.&#xA;Setting up inference with pre-trained weights GPT2 has four models from 124M to 1.</description>
    </item>
    <item>
      <title>TinyGrad GPT2 à la Karpathy - Part 2</title>
      <link>http://localhost:1313/post/mini_projects/gpt2_p2/</link>
      <pubDate>Tue, 11 Jun 2024 16:25:35 +0200</pubDate>
      <guid>http://localhost:1313/post/mini_projects/gpt2_p2/</guid>
      <description>As expected, this part of the series has been more difficult. Karpathy makes use of lots of relatively niche, PyTorch specific functions that don&amp;rsquo;t have built-in support in TinyGrad. As a result, I need to be more methodical about how I approach this and rigorously confirm that things are working as expected.&#xA;I&amp;rsquo;ll start with the code we left off on in part 1.&#xA;from tinygrad import Tensor, dtypes from tinygrad.</description>
    </item>
    <item>
      <title>Simple Symbolic Distillation on XOR Neural Network</title>
      <link>http://localhost:1313/post/symbolic_regression/symbolic_distillation/</link>
      <pubDate>Sun, 09 Jun 2024 21:25:23 +0200</pubDate>
      <guid>http://localhost:1313/post/symbolic_regression/symbolic_distillation/</guid>
      <description>What is symbolic distillation? Symbolic distillation is a technique pioneered by Miles Cranmer to improve interpretability of neural networks using symbolic regression. The basic premise is to approximate various components of a neural network by applying symbolic regression to different parts. These parts can then be substituted for the original network.&#xA;We&amp;rsquo;ll try this</description>
    </item>
    <item>
      <title>Learning PySr for Symbolic Regression</title>
      <link>http://localhost:1313/post/symbolic_regression/learning_pysr/</link>
      <pubDate>Sun, 09 Jun 2024 13:17:27 +0200</pubDate>
      <guid>http://localhost:1313/post/symbolic_regression/learning_pysr/</guid>
      <description>What is PySr? A library for symbolic regression made by one of my favorite researchers, Miles Cranmer. The backend genetic algorithm(optimization method for finding symbolic expression) runs on Julia, so it&amp;rsquo;s really fast. It&amp;rsquo;s also extremely flexible.&#xA;Installing and Running PySr I&amp;rsquo;m going to use a Jupyter notebook with a virtualenv environment for all my PySr experiments.&#xA;virtualenv venv source venv/bin/activate pip install numpy pysr Vector-&amp;gt;Scalar Fit The first import of pysr will take much longer than the next as Julia dependencies are getting set up.</description>
    </item>
    <item>
      <title>XOR</title>
      <link>http://localhost:1313/post/mini_projects/xor/</link>
      <pubDate>Sat, 08 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/mini_projects/xor/</guid>
      <description>The tiniest project &amp;ndash; teaching a neural net to XOR. Obviously, I&amp;rsquo;m doing this in tinygrad because it&amp;rsquo;s tiny.&#xA;A B A XOR B 0 0 0 0 1 1 1 0 1 1 1 0 First, imports.&#xA;from tinygrad import Tensor, TinyJit from tinygrad.nn.optim import SGD import numpy as np from tqdm import tqdm, trange import matplotlib.pyplot as plt Next, dataset. It&amp;rsquo;s the same as the truth table.&#xA;x = Tensor([[0,0],[0,1],[1,0],[1,1]]) y = Tensor([[0],[1],[1],[0]]) x.</description>
    </item>
    <item>
      <title>Teaching a GPT to do Modular Addition</title>
      <link>http://localhost:1313/post/grokking_squared/part1/</link>
      <pubDate>Sun, 26 May 2024 21:07:39 +0200</pubDate>
      <guid>http://localhost:1313/post/grokking_squared/part1/</guid>
      <description>Prerequisite Terminology Grok (verb) - To grok means to fully and intuitively understand. When you finally grok something, it&amp;rsquo;s an Aha moment where all the lingering questions you&amp;rsquo;ve had are suddenly resolved. For the purposes of this article, I don&amp;rsquo;t need to go any deeper than that. Everybody knows that feeling.&#xA;Mechanistic Interpretability (noun) - A sub-field of deep learning studying the algorithms learned by neural-net-based architectures. It was popularized (and coined?</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>I&amp;rsquo;m a third-year student studying CS in the Engineering School at the University of Virginia.&#xA;zur3hn@virginia.edu Here&amp;rsquo;s my resume. Here&amp;rsquo;s my linkedin that I need to update. </description>
    </item>
  </channel>
</rss>
